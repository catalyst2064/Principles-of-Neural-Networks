{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwKgALfy4fDU"
   },
   "source": [
    "# ライブラリ，ユーティリティ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-WlYg-9VfgQ"
   },
   "source": [
    "## インストール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "$ pip install git+https://github.com/BindsNET/bindsnet.git\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FU4mPgFxq0V3"
   },
   "source": [
    "・Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5wuNwvrVp2e",
    "outputId": "9056f26b-215f-43a9-8f9e-27d639a4fbae"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install git+https://github.com/BindsNET/bindsnet.git\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z42ziSnPgE7_"
   },
   "source": [
    "## インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "xEqt8DpUThYw",
    "outputId": "941af613-a890-4449-f1e3-700adbda4b19"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# インポート\n",
    "#-------------------------------------------------\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from time import time as t\n",
    "import warnings\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#import tensorflow_datasets as tfds                # データセット（MNIST, Iris）\n",
    "from torchvision.datasets import MNIST            # データセット（MNIST）\n",
    "from sklearn.datasets import load_digits          # データセット（digits）\n",
    "from sklearn.datasets import load_iris            # データセット（Iris）\n",
    "\n",
    "from bindsnet.models import DiehlAndCook2015      # Diehl&Cook(2015)モデル\n",
    "from bindsnet.evaluation import all_activity, assign_labels\n",
    "from bindsnet.network.monitors import Monitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXwlDPAjFOuc"
   },
   "source": [
    "## 環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKXDghX9FKKF"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# 環境設定\n",
    "#-------------------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "np.set_printoptions(formatter={\"float\":\"{:7.4f}\".format})   # 数値の表示を整える\n",
    "%config InlineBackend.figure_format = \"retina\"              # 図の描画を高精細に\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORPgLyr4eJTY"
   },
   "source": [
    "## ユーティリティ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E66rwlPWeM2h"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ユーティリティ\n",
    "#-------------------------------------------------\n",
    "\n",
    "def reset_seed(seed = 3407):                      # 乱数シードをリセット\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def shuffle(data, label):                         # 要素の対応関係を保ったままシャッフル\n",
    "    zipped = list(zip(data, label))\n",
    "    np.random.shuffle(zipped)\n",
    "    tupple = list(zip(*zipped))\n",
    "    return (np.array(tupple[0]), np.array(tupple[1]))\n",
    "\n",
    "def vector_to_matrix(vector):                     # ベクトルを強制的に行列に変換\n",
    "    length = len(vector)                          # ベクトルの長さを取得\n",
    "    cols = int(np.ceil(np.sqrt(length)))          # 列のサイズを決定\n",
    "    rows = cols - 1 if (cols - 1) * cols >= length else cols   # 行を列よりも1つ少なくしても入り切るか？\n",
    "    matrix = np.full((rows, cols), None)          # Noneで埋められた新しい行列を作成\n",
    "    matrix.flat[:length] = vector\n",
    "    return matrix\n",
    "\n",
    "def judge(flag):                                  # 論理値を判定文字列に変換\n",
    "    return \"✓ PASS\" if flag else \"✗ FAIL\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtbB-BLgQxOr"
   },
   "source": [
    "# データセットを内部表現に変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prO3jkp6YwzD"
   },
   "source": [
    "## スケーラ（Scaler）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrdy8U1UY1ec"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# スケーラ\n",
    "#-------------------------------------------------\n",
    "\n",
    "class Scaler():\n",
    "    def __init__(self, axis=None):\n",
    "        self.axis = axis\n",
    "\n",
    "    @staticmethod\n",
    "    def passthrough_scaler(data, axis):           # パススルー\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def z_score_scaler(data, axis):               # z-score正規化\n",
    "        mean = data.mean(axis=axis, keepdims=True)\n",
    "        std  = data.std (axis=axis, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "\n",
    "    @staticmethod\n",
    "    def min_max_scaler(data, axis):               # Min-Max正規化\n",
    "        min = data.min(axis=axis, keepdims=True)\n",
    "        max = data.max(axis=axis, keepdims=True)\n",
    "        return (data - min) / (max - min)\n",
    "\n",
    "    def passthrough(self, data):                  # パススルー\n",
    "        return self.passthrough_scaler(data, self.axis)\n",
    "\n",
    "    def z_score(self, data):                      # z-score正規化\n",
    "        return self.z_score_scaler(data, self.axis)\n",
    "\n",
    "    def min_max(self, data):                      # Min-Max正規化\n",
    "        return self.min_max_scaler(data, self.axis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUH__QsuY2HB"
   },
   "source": [
    "## ニューラルエンコーダ（Neural Encoder）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdiYf7MAY6Uo"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ニューラルエンコーダ\n",
    "#-------------------------------------------------\n",
    "\n",
    "class NeuralEncoder():\n",
    "    def __init__(self, Hypr):\n",
    "        self.lamb     = Hypr.lamb\n",
    "        self.duration = Hypr.duration\n",
    "        self.dt       = Hypr.dt\n",
    "\n",
    "    @staticmethod\n",
    "    def poisson_simple_encoder(scaled, lamb, duration, dt):     # ポアソンエンコーダ（簡易版）\n",
    "        steps  = int(duration / dt)\n",
    "        scaled = scaled[:, np.newaxis]\n",
    "        lamb   = scaled * lamb * dt\n",
    "        dist   = np.random.poisson(lamb, (scaled.size, steps))\n",
    "        return (dist > 0).astype(int)\n",
    "\n",
    "    @staticmethod\n",
    "    def poisson_formal_encoder(scaled, lamb, duration, dt):     # ポアソンエンコーダ\n",
    "        pass                                                    # 未実装\n",
    "\n",
    "    def poisson_simple(self, scaled):                           # ポアソンエンコーダ（簡易版）\n",
    "        return self.poisson_simple_encoder(scaled, self.lamb, self.duration, self.dt)\n",
    "\n",
    "    def poisson_formal(self, scaled):                           # ポアソンエンコーダ\n",
    "        return self.poisson_formal_encoder(scaled, self.lamb, self.duration, self.dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGWWnff_Rqyr"
   },
   "source": [
    "## サブセット（Subset）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oD8oke7uRbHj"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# サブセットの基底クラス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SubsetBase(metaclass=ABCMeta):\n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler                      # 生データの正規化に使用するスケーラ\n",
    "        self.data   = None                        # 生データ\n",
    "        self.scaled = None                        # 正規化されたデータ\n",
    "        self.label  = None                        # 正解ラベル\n",
    "\n",
    "    def __call__(self, data, label, i, j):        # サブセットを構築\n",
    "        self.data  = data [i:j]\n",
    "        self.label = label[i:j]\n",
    "        self.scaling_coding()\n",
    "\n",
    "    @abstractmethod\n",
    "    def scaling_coding(self):                     # スケーラとエンコーダを適用\n",
    "        pass\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANNサブセット\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnSubset(SubsetBase):\n",
    "    def __init__(self, scaler, classes):\n",
    "        super().__init__(scaler)\n",
    "        self.onehot  = None                       # One-Hotラベル\n",
    "        self.classes = classes\n",
    "\n",
    "    @staticmethod\n",
    "    def onehot_encoder(label, classes):           # One-Hotエンコーダ\n",
    "        onehot = np.zeros((label.size, classes))\n",
    "        onehot[np.arange(label.size), label] = 1.0\n",
    "        return onehot\n",
    "\n",
    "    def scaling_coding(self):\n",
    "        self.scaled = self.scaler(self.data)\n",
    "        self.onehot = self.onehot_encoder(self.label, self.classes)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# SNNサブセット\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SnnSubset(SubsetBase):\n",
    "    def __init__(self, scaler, encoder):\n",
    "        super().__init__(scaler)\n",
    "        self.encoder = encoder                    # 選択されたエンコーダ\n",
    "        self.spiket  = None                       # スパイクトレイン\n",
    "\n",
    "    def scaling_coding(self):\n",
    "        self.scaled = self.scaler(self.data)\n",
    "        self.spiket = np.array([self.encoder(x).T for x in self.scaled.copy()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh0GBk8qWSaX"
   },
   "source": [
    "## スーパーセット（Superset）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EzT5vqJZn6N"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# スーパーセットの基底クラス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SupersetBase(metaclass=ABCMeta):\n",
    "    def load(self, dataset, extract):\n",
    "        (data, label) = dataset(extract)          # データセットを読み込む\n",
    "        (data, label) = shuffle(data, label)      # データセット全体をシャッフル\n",
    "        (ex0, ex1, ex2) = extract                 # 抽出サイズ\n",
    "        self.train(data, label, 0, ex0)\n",
    "        self.valid(data, label, ex0, ex0+ex1)\n",
    "        self.test (data, label, ex0+ex1, ex0+ex1+ex2)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANNスーパーセット\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnSuperset(SupersetBase):\n",
    "    def __init__(self, scaler, classes):\n",
    "        self.train = AnnSubset(scaler, classes)   # トレーニングセット\n",
    "        self.valid = AnnSubset(scaler, classes)   # バリデーションセット\n",
    "        self.test  = AnnSubset(scaler, classes)   # テストセット\n",
    "\n",
    "#-------------------------------------------------\n",
    "# SNNスーパーセット\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SnnSuperset(SupersetBase):\n",
    "    def __init__(self, scaler, encoder):\n",
    "        self.train = SnnSubset(scaler, encoder)   # トレーニングセット\n",
    "        self.valid = SnnSubset(scaler, encoder)   # バリデーションセット\n",
    "        self.test  = SnnSubset(scaler, encoder)   # テストセット\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nau16sBmZRhS"
   },
   "source": [
    "## データセット（Dataset）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSrYJx2GWWrY"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# データセットの基底クラス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class DatasetBase(metaclass=ABCMeta):\n",
    "    @staticmethod\n",
    "    def bulk_load(dataset, count):                # 指定された件数のデータを一括で読み込む\n",
    "        data, label = [], []                      #（tensorflow_datasets専用）\n",
    "        dataset = tfds.as_numpy(dataset.take(count))\n",
    "        for (dt, lb) in dataset:\n",
    "            data.append(dt)\n",
    "            label.append(lb)\n",
    "        return (np.array(data), np.array(label))\n",
    "\n",
    "#-------------------------------------------------\n",
    "# MNISTデータセット\n",
    "#-------------------------------------------------\n",
    "\n",
    "class DatasetMNIST(DatasetBase):\n",
    "    class attr():\n",
    "        __maxrec = 60000                          # 最大60,000件\n",
    "        inputs   = 784                            # 画素数（28×28）\n",
    "        shape    = (1, 28, 28)\n",
    "        classes  = 10                             # 0-9\n",
    "    \"\"\"\n",
    "    def __call__(self, extract):                  # データセットを読み込む（tensorflow_datasetsから）\n",
    "        dataset = tfds.load(\"mnist\", as_supervised=True)\n",
    "        (data, label) = self.bulk_load(dataset[\"train\"], sum(extract))\n",
    "        data = data.reshape(-1, self.attr.inputs)\n",
    "        return (data, label)\n",
    "    \"\"\"\n",
    "    def __call__(self, extract):                  # データセットを読み込む（torchvision.datasetsから）\n",
    "        dataset = MNIST(root=\"./dataset\", train=True, download=True)\n",
    "        data    = dataset.data.numpy().reshape(-1, self.attr.inputs)\n",
    "        label   = dataset.targets.numpy()\n",
    "        return (data, label)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# digitsデータセット\n",
    "#-------------------------------------------------\n",
    "\n",
    "class DatasetDigits(DatasetBase):\n",
    "    class attr():\n",
    "        __maxrec = 1797                           # 最大1,797件\n",
    "        inputs   = 64                             # 画素数（8×8）\n",
    "        shape    = (1, 8, 8)\n",
    "        classes  = 10                             # 0-9\n",
    "\n",
    "    def __call__(self, extract):                  # データセットを読み込む（sklearn.datasetsから）\n",
    "        dataset = load_digits()\n",
    "        return (dataset.data, dataset.target)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Irisデータセット\n",
    "#-------------------------------------------------\n",
    "\n",
    "class DatasetIris(DatasetBase):\n",
    "    class attr():\n",
    "        __maxrec = 150                            # 最大150件\n",
    "        inputs   = 4                              # 4つの特徴量\n",
    "        shape    = (1, 4)\n",
    "        classes  = 3                              # 3品種\n",
    "    \"\"\"\n",
    "    def __call__(self, extract):                  # データセットを読み込む（tensorflow_datasetsから）\n",
    "        dataset = tfds.load(\"iris\", as_supervised=True)\n",
    "        (data, label) = self.bulk_load(dataset[\"train\"], sum(extract))\n",
    "        return (data, label)\n",
    "    \"\"\"\n",
    "    def __call__(self, extract):                  # データセットを読み込む（sklearn.datasetsから）\n",
    "        dataset = load_iris()\n",
    "        return (dataset.data, dataset.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUXjzdzLYAMJ"
   },
   "source": [
    "# 観察ツール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bYqcNfhlGex"
   },
   "source": [
    "## サブセットビューア（Subset Viewer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMkO_TYRk_AM"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# サブセットビューア\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SubsetViewer():\n",
    "    def __init__(self, subset, shape):\n",
    "        self.data   = subset.data\n",
    "        self.scaled = subset.scaled\n",
    "        self.label  = subset.label\n",
    "        self.shape  = (-1,) + shape[1:]\n",
    "\n",
    "    def show_raw_data(self, i, j):                # 原本の特徴量データを表示\n",
    "        print(self.data[i:j].reshape(self.shape))\n",
    "\n",
    "    def show_raw_scaled(self, i, j):              # スケーリング済みの特徴量データを表示\n",
    "        print(self.scaled[i:j].reshape(self.shape))\n",
    "\n",
    "    def show_img_data(self, i, j, fs=(6,4)):      # 特徴量データをグレー画像として表示\n",
    "        data  = self.data[i:j].reshape(self.shape)\n",
    "        label = self.label[i:j]\n",
    "        plt.figure(figsize=fs)\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        for (x, (data, label)) in enumerate(zip(data, label)):\n",
    "            plt.subplot(2, 5, x + 1)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(label)\n",
    "            plt.imshow(data, vmin=data.min(), vmax=data.max(), cmap=\"Greys\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIYZTzVhnRve"
   },
   "source": [
    "## スパイクビューア（Spike Viewer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSCH-U3HlFO_"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# スパイクビューア\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SpikeViewer():\n",
    "    def __init__(self, spiket):\n",
    "        self.spiket = spiket.T\n",
    "\n",
    "    def show_bit_pattern(self):                   # ビットパターンを表示\n",
    "        display(pd.DataFrame(self.spiket))\n",
    "\n",
    "    def show_raster_plot(self, fig=(8,4)):        # ラスタープロットを表示\n",
    "        plt.figure(figsize=fig)\n",
    "        x_axis = range(self.spiket.shape[1])\n",
    "        raster = np.where(self.spiket == 0, np.nan, self.spiket) # 0をNaNに置換\n",
    "        for (i, raster) in enumerate(raster):\n",
    "            plt.scatter(x_axis, raster*i, s=1.0, c=\"mediumblue\")\n",
    "        yticks = plt.gca().get_yticks().astype(int)\n",
    "        plt.yticks(yticks, yticks)\n",
    "        plt.xlabel(\"Step #\")\n",
    "        plt.ylabel(\"Neuron #\")\n",
    "        plt.show()\n",
    "\n",
    "    def show_spike_train(self, i, j, fs=(8,4)):   # スパイクトレインを表示\n",
    "        plt.figure(figsize=fs)\n",
    "        plt.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "        event = [np.where(_)[0] for _ in self.spiket[i:j]]  # スパイクイベントを収集\n",
    "        plt.eventplot(event, linelengths=0.5, colors=\"mediumblue\")\n",
    "        yticks = plt.gca().get_yticks().astype(int)\n",
    "        plt.yticks(yticks, yticks + i)\n",
    "        plt.xlabel(\"Step #\")\n",
    "        plt.ylabel(\"Neuron #\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GR6PhRI49Jv"
   },
   "source": [
    "# 評価指標と性能評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y22sYTo5hzm"
   },
   "source": [
    "## メトリクス（Metrics）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jb1Ep3dV5rNs"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ANNメトリクス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnMetrics():\n",
    "    def __init__(self, Fn_loss):\n",
    "        self.Fn_loss = Fn_loss                    # 損失関数\n",
    "        self.loss = []                            # 損失\n",
    "        self.accu = []                            # 正解率\n",
    "\n",
    "    def append(self, pred, onehot):               # 測定結果を追加（1エポックごと）\n",
    "        self.loss.append(self.Fn_loss(pred, onehot))\n",
    "        self.accu.append(ann_accuracy(pred, onehot))\n",
    "\n",
    "#-------------------------------------------------\n",
    "# SNNメトリクス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SnnMetrics():\n",
    "    def __init__(self):\n",
    "        self.accu  = []                           # 正解率\n",
    "\n",
    "    def append(self, pred, label):                # 測定結果を追加（1エポックごと）\n",
    "        self.accu.append(snn_accuracy(pred, label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olTutEjzSJOC"
   },
   "source": [
    "## メジャー（Measure）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nhhhV5JH17L"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# メジャーの基底クラス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class MeasureBase(metaclass=ABCMeta):\n",
    "    def init(self, owner, epochs, verbose):\n",
    "        self.owner   = owner\n",
    "        self.epochs  = epochs\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @abstractmethod\n",
    "    def plot_metrics(self):                       # 評価指標をプロット\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def show_metrics(self):                       # 評価指標を表示\n",
    "        pass\n",
    "\n",
    "    def show_each_epoch(self, step, time):\n",
    "        print(f\"epoch  = {step+1:>3d} / {self.epochs:>3d}, \"\n",
    "              f\"train accu = {self.train.accu[-1]:.3f}, \"\n",
    "              f\"valid accu = {self.valid.accu[-1]:.3f}, \"\n",
    "              f\"time = {time:.1f} sec（{round(time/60)} min）\")\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANNメジャー\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnMeasure(MeasureBase):\n",
    "    def __init__(self, Fn_loss):\n",
    "        self.train = AnnMetrics(Fn_loss)          # 測定結果（トレーニングセット）\n",
    "        self.valid = AnnMetrics(Fn_loss)          # 測定結果（バリデーションセット）\n",
    "\n",
    "    def __call__(self, epoch, time):\n",
    "        subset = self.owner.superset.train        # トレーニングセットを測定\n",
    "        pred = self.owner.predict(subset.scaled)\n",
    "        self.train.append(pred, subset.onehot)\n",
    "        subset = self.owner.superset.valid        # バリデーションセットを測定\n",
    "        pred = self.owner.predict(subset.scaled)\n",
    "        self.valid.append(pred, subset.onehot)\n",
    "        if self.verbose: self.show_each_epoch(epoch, time)\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        plt.ylim(0.0, 1.2)\n",
    "        plt.grid()\n",
    "        plt.plot(self.train.accu, label=\"train accu\")\n",
    "        plt.plot(self.valid.accu, label=\"valid accu\")\n",
    "        plt.plot(self.train.loss, label=\"train loss\")\n",
    "        plt.plot(self.valid.loss, label=\"valid loss\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def show_metrics(self):\n",
    "        print(\"Metrics：\")\n",
    "        print(f\"  train accu = {self.train.accu[-1]:.3f}\")\n",
    "        print(f\"  valid accu = {self.valid.accu[-1]:.3f}\")\n",
    "        print(f\"  train loss = {self.train.loss[-1]:.3f}\")\n",
    "        print(f\"  valid loss = {self.valid.loss[-1]:.3f}\")\n",
    "\n",
    "#-------------------------------------------------\n",
    "# SNNメジャー\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SnnMeasure(MeasureBase):\n",
    "    def __init__(self):\n",
    "        self.train = SnnMetrics()                 # 測定結果（トレーニングセット）\n",
    "        self.valid = SnnMetrics()                 # 測定結果（バリデーションセット）\n",
    "\n",
    "    def __call__(self, epoch, time):\n",
    "        limit = 50                                # SNNは遅いので測定件数を50件に制限\n",
    "        subset = self.owner.superset.train        # トレーニングセットを測定\n",
    "        pred = self.owner.predict(subset.spiket[0:limit])\n",
    "        self.train.append(pred, subset.label[0:limit])\n",
    "        subset = self.owner.superset.valid        # バリデーションセットを測定\n",
    "        pred = self.owner.predict(subset.spiket[0:limit])\n",
    "        self.valid.append(pred, subset.label[0:limit])\n",
    "        if self.verbose: self.show_each_epoch(epoch, time)\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        plt.ylim(0.0, 1.2)\n",
    "        plt.grid()\n",
    "        plt.plot(self.train.accu, label=\"train accu\")\n",
    "        plt.plot(self.valid.accu, label=\"valid accu\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def show_metrics(self):\n",
    "        print(\"Metrics：\")\n",
    "        print(f\"  train accu = {self.train.accu[-1]:.3f}\")\n",
    "        print(f\"  valid accu = {self.valid.accu[-1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrcSVPYXDnlO"
   },
   "source": [
    "## 暫定正解率（Provisional Accuracy）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YqDkVRs_h62"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# 暫定正解率\n",
    "#-------------------------------------------------\n",
    "\n",
    "class Provaccu():\n",
    "    def __init__(self, epochs, total):\n",
    "        self.epochs = epochs\n",
    "        self.total = total\n",
    "        self.reset(None)\n",
    "\n",
    "    def reset(self, epoch):                       # 暫定正解率の算出用データを初期化\n",
    "        self.epoch = epoch\n",
    "        self.correctans = 0                       # 暫定正解数の累計\n",
    "        self.cumulative = 0                       # 暫定データ数の累計\n",
    "\n",
    "    def update(self, pred, labels):               # 暫定正解率の算出用データを更新\n",
    "        self.correctans += np.sum(pred == labels)\n",
    "        self.cumulative += len(labels)\n",
    "\n",
    "    def show_prov_accu(self, step, time):         # 暫定正解率を表示\n",
    "        accu = round(self.correctans / self.cumulative, 4)\n",
    "        print(f\"update = {step + 1:>3d} / {self.total}, \"\n",
    "              f\"accu = {accu:.3f}, \"\n",
    "              f\"time = {time:.1f} sec（{round(time/60)} min）\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbTwYlVriiqJ"
   },
   "source": [
    "## パフォーマンス（Performance）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmsfJREMSNw4"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# パフォーマンスの基底クラス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class PerformBase(metaclass=ABCMeta):\n",
    "    def __init__(self, owner):\n",
    "        self.owner = owner\n",
    "\n",
    "    @staticmethod\n",
    "    def show_judge(step, pred, label):            # 判定結果を表示\n",
    "        matched = pred == label\n",
    "        print(f\"data = {step:>2d}, \"\n",
    "              f\"pred = {pred}, label = {label}, \"\n",
    "              f\"judge = {judge(matched)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def show_accuracy(accu):                      # 正解率を表示\n",
    "        print(f\"test accu = {accu:.3f}\")\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANNパフォーマンス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnPerform(PerformBase):\n",
    "    def __call__(self, subset):                   # 判定結果と正解率を表示\n",
    "        pred = self.owner.predict(subset.scaled)\n",
    "        for step, (p, o) in enumerate(zip(pred, subset.onehot)):\n",
    "            self.show_judge(step, np.argmax(p), np.argmax(o))\n",
    "        self.show_accuracy(ann_accuracy(pred, subset.onehot))\n",
    "\n",
    "#-------------------------------------------------\n",
    "# SNNパフォーマンス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SnnPerform(PerformBase):\n",
    "    def __call__(self, subset):                   # 判定結果と正解率を表示\n",
    "        pred = self.owner.predict(subset.spiket)\n",
    "        for step, (p, l) in enumerate(zip(pred, subset.label)):\n",
    "            self.show_judge(step, p, l)\n",
    "        self.show_accuracy(snn_accuracy(pred, subset.label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDahdq32mfWF"
   },
   "source": [
    "# ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BujVdrhEAkqj"
   },
   "source": [
    "## 関数群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0JtY6odGAKN"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ANN初期化関数（initialization function）        → Fn_init\n",
    "#-------------------------------------------------\n",
    "\n",
    "def glorot_uniform(rows, cols):                   # glorot uniform\n",
    "    min = -np.sqrt(6 / (rows + cols))\n",
    "    max =  np.sqrt(6 / (rows + cols))\n",
    "    return np.random.uniform(min, max, (rows, cols))\n",
    "\n",
    "def he_normal(rows, cols):                        # he normal (for ReLU)\n",
    "    std = np.sqrt(2 / cols)\n",
    "    return np.random.normal(0.0, std, (rows, cols))\n",
    "\n",
    "def xavier_normal(rows, cols):                    # xavier normal (for sigmoid, tanh)\n",
    "    std = 1 / np.sqrt(cols)\n",
    "    return np.random.normal(0.0, std, (rows, cols))\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANN活性化関数（activation function）            → Fn_actv\n",
    "#-------------------------------------------------\n",
    "\n",
    "def sigmoid(x):                                   # sigmoid関数\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):                                      # tanh関数\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):                                      # ReLU関数\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):                                   # softmax関数\n",
    "    axis = 0 if (x.ndim == 1) else 1                    # 0：ベクトル対応，1：行列対応\n",
    "    exps = np.exp(x - x.max(axis=axis, keepdims=True))  # オーバーフロー対策\n",
    "    return exps / exps.sum(axis=axis, keepdims=True)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANN導関数（derivative function）                → Fn_drvt\n",
    "#-------------------------------------------------\n",
    "\n",
    "def sigmoid_der(x):                               # sigmoid関数の導関数\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
    "\n",
    "def tanh_der(x):                                  # tanh関数の導関数\n",
    "    return 1.0 - (tanh(x) ** 2)\n",
    "\n",
    "def relu_der(x):                                  # ReLU関数の導関数\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANN損失関数（loss function）                    → Fn_loss\n",
    "#-------------------------------------------------\n",
    "def crossentropy(y, t):                           # 交差エントロピー損失関数\n",
    "    y = np.clip(y, 1e-15, 1 - 1e-15)\n",
    "    return -np.sum(t * np.log(y), axis=1).mean()\n",
    "\n",
    "#-------------------------------------------------\n",
    "# 評価関数\n",
    "#-------------------------------------------------\n",
    "\n",
    "def ann_accuracy(y, t):                           # ANN正解率関数\n",
    "    return np.mean(np.argmax(y, axis=1) == np.argmax(t, axis=1))\n",
    "\n",
    "def snn_accuracy(y, t):                           # SNN正解率関数\n",
    "    return np.mean(y == t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsE1Mt8bHErR"
   },
   "source": [
    "## ANNニューロン（ANN Neuron）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqz1RLqyOk3y"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ANNニューロン\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnNeuron():\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W                                # 重み（ベクトル）\n",
    "        self.b = b                                # バイアス（1個）\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):                              # ミニバッチの開始前にリセットが必要\n",
    "        self.W_grad = []                          # 重みの勾配の蓄積用\n",
    "        self.b_grad = []                          # バイアスの勾配の蓄積用\n",
    "\n",
    "    def weighted_sum(self, X):                    # 総入力関数\n",
    "        return X @ self.W + self.b                # 式 (5.1) 総入力値\n",
    "\n",
    "    def calc_grad(self, X, delta):\n",
    "        self.W_grad.append(X * delta)             # 式 (5.6) 重みの勾配\n",
    "        self.b_grad.append(delta)                 # 式 (5.7) バイアスの勾配\n",
    "        return self.W * delta                     # 式 (5.5) 入力の勾配（レイヤで結線どおりに合算する）\n",
    "\n",
    "    def update(self, eta):                        # パラメータを更新\n",
    "        self.W -= np.sum(self.W_grad, axis=0) * eta  # 式 (5.8) 重みの更新\n",
    "        self.b -= np.sum(self.b_grad, axis=0) * eta  # 式 (5.9) バイアスを更新\n",
    "        self.reset()                              # 次のミニバッチに備える\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05RB9orwOkRg"
   },
   "source": [
    "## ANNレイヤ（ANN Layer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJ-L6D3SIoKB"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ANNレイヤの基底クラス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnLayerBase(metaclass=ABCMeta):\n",
    "    def __init__(self, funcs):\n",
    "        (self.Fn_init, self.Fn_actv, self.Fn_drvt) = funcs\n",
    "\n",
    "    def calc_delta(self, X_grad):\n",
    "        return X_grad * self.Fn_drvt(self.U)      # 式 (5.4),(7.4)  誤差の勾配（中間層）\n",
    "\n",
    "    @abstractmethod\n",
    "    def init(self, rows, cols):                   # パラメータの初期化\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def fire(self, X):                            # レイヤからの出力を計算\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def calc_grad(self, delta):                   # 入力の勾配を計算\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, eta):                        # パラメータを更新\n",
    "        pass\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANNレイヤ（ニューロン版）\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnLayerNeuron(AnnLayerBase):\n",
    "    def __init__(self, rows, cols, funcs):\n",
    "        super().__init__(funcs)\n",
    "        self.neurons = []                         # ニューロンの一覧\n",
    "        self.init(rows, cols)\n",
    "\n",
    "    def init(self, rows, cols):                   # ニューロンの初期化と一覧への追加\n",
    "        for weight in self.Fn_init(rows+1, cols).T:\n",
    "            neuron = AnnNeuron(weight[0:rows], weight[-1])\n",
    "            self.neurons.append(neuron)\n",
    "\n",
    "    def fire(self, X):\n",
    "        U = [neuron.weighted_sum(X) for neuron in self.neurons]\n",
    "        self.X = X                                # レイヤへの入力を保存（逆伝播で使用）\n",
    "        self.U = np.array(U)                      # ニューロンからの出力（逆伝播で使用）\n",
    "        return self.Fn_actv(self.U)               # 式 (5.2) 活性化値\n",
    "\n",
    "    def calc_grad(self, delta):\n",
    "        X_grad = []                               # 入力の勾配を蓄積\n",
    "        for (neuron, delta) in zip(self.neurons, delta):\n",
    "            X_grad.append(neuron.calc_grad(self.X, delta))\n",
    "        return np.sum(X_grad, axis=0)             # 蓄積した入力の勾配をリンクの結線どおりに合算\n",
    "\n",
    "    def update(self, eta):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.update(eta)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANNレイヤ（行列版）\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnLayerMatrix(AnnLayerBase):\n",
    "    def __init__(self, rows, cols, funcs):\n",
    "        super().__init__(funcs)\n",
    "        self.init(rows, cols)\n",
    "\n",
    "    def init(self, rows, cols):\n",
    "        weight = self.Fn_init(rows+1, cols)\n",
    "        (self.W, self.b) = (weight[0:rows], weight[-1])\n",
    "\n",
    "    def fire(self, X):\n",
    "        self.X = X\n",
    "        self.U = X @ self.W + self.b              # 式 (7.1) 総入力値\n",
    "        return self.Fn_actv(self.U)               # 式 (7.2) 活性化値\n",
    "\n",
    "    def calc_grad(self, delta):\n",
    "        self.W_grad = self.X.T @ delta            # 式 (7.6) 重みの勾配\n",
    "        self.b_grad = np.sum(delta, axis=0)       # 式 (7.7) バイアスの勾配\n",
    "        return delta @ self.W.T                   # 式 (7.5) 入力の勾配\n",
    "\n",
    "    def update(self, eta):\n",
    "        self.W -= eta * self.W_grad               # 式 (7.8) 重みの更新\n",
    "        self.b -= eta * self.b_grad               # 式 (7.9) バイアスの更新\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPsvhkOrIQIs"
   },
   "source": [
    "## ANNネットワーク（ANN Network）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUlTx_dWn6w6"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ANNネットワークの基底クラス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnNetworkBase(metaclass=ABCMeta):\n",
    "    def __init__(self, Layer, cols, eta):\n",
    "        self.Layer = Layer                        # レイヤ・クラスを確定\n",
    "        self.rows = cols                          # 最初のレイヤの行数を確定\n",
    "        self.eta = eta                            # 学習率\n",
    "        self.layers = []                          # レイヤの一覧\n",
    "\n",
    "    def add(self, cols, funcs):                   # レイヤを追加\n",
    "        layer = self.Layer(self.rows, cols, funcs)\n",
    "        self.layers.append(layer)\n",
    "        self.rows = cols                          # 次のレイヤの行数を確定\n",
    "\n",
    "    def forward(self, X):                         # 順伝播\n",
    "        for layer in self.layers:\n",
    "            X = layer.fire(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, delta):                    # 逆伝播\n",
    "        layers = list(reversed(self.layers))\n",
    "        X_grad = layers[0].calc_grad(delta)\n",
    "        for layer in layers[1:]:\n",
    "            delta  = layer.calc_delta(X_grad)\n",
    "            X_grad = layer.calc_grad(delta)\n",
    "\n",
    "    def update(self):                             # パラメータを更新\n",
    "        for layer in self.layers:\n",
    "            layer.update(self.eta)\n",
    "\n",
    "    @abstractmethod\n",
    "    def batch(self, ss, mini):\n",
    "        pass\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANNネットワーク（ニューロン版）\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnNetworkNeuron(AnnNetworkBase):\n",
    "    def __init__(self, cols, eta):\n",
    "        super().__init__(AnnLayerNeuron, cols, eta)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = []\n",
    "        for x in X:                               # 行列から1行ずつ取り出して処理する\n",
    "            Y.append(super().forward(x))\n",
    "        return np.array(Y)\n",
    "\n",
    "    def batch(self, ss, mini):\n",
    "        for i in mini:                            # 行列から1行ずつ取り出して処理する\n",
    "            Y = super().forward(ss.scaled[i])\n",
    "            self.backward(Y - ss.onehot[i])       # 式 (5.3) 誤差の勾配（出力層）\n",
    "        self.update()\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANNネットワーク（行列版）\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnNetworkMatrix(AnnNetworkBase):\n",
    "    def __init__(self, cols, eta):\n",
    "        super().__init__(AnnLayerMatrix, cols, eta)\n",
    "\n",
    "    def batch(self, ss, mini):\n",
    "        Y = self.forward(ss.scaled[mini])\n",
    "        self.backward(Y - ss.onehot[mini])        # 式 (7.3) 誤差の勾配（出力層）\n",
    "        self.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP1t4fVx3ysw"
   },
   "source": [
    "## ラベルマップ（Label Map）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61XV0rSh33iC"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ラベルマップ（Label Map）\n",
    "#-------------------------------------------------\n",
    "\n",
    "class LabelMap():\n",
    "    def __init__(self, neurons):\n",
    "        self.data = -torch.ones(neurons)          # ラベルマップを保持\n",
    "\n",
    "    def show_label_map(self):                     # ラベルマップを表示\n",
    "        labelmap = self.data.numpy()              # torch→numpy変換\n",
    "        rect = vector_to_matrix(labelmap)         # ラベルマップを矩形に変換\n",
    "        display(pd.DataFrame(rect).replace({None:\"-\"}))\n",
    "\n",
    "    def show_label_count(self, fs=(5,4)):         # ラベルの出現頻度を表示\n",
    "        labelmap = self.data.numpy()              # torch→numpy変換\n",
    "        plt.figure(figsize=fs)\n",
    "        label, counts = np.unique(labelmap, return_counts=True)\n",
    "        plt.bar(label, counts, align=\"center\")\n",
    "        for (i, count) in enumerate(counts):\n",
    "            plt.text(i, count, str(count), ha=\"center\", va=\"bottom\")\n",
    "        plt.xlabel(\"Label\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(label)\n",
    "        plt.show()\n",
    "\n",
    "    def show_pred_rate(self, res):                # 予測レートを表示\n",
    "        labelmap = self.data.numpy()              # torch→numpy変換\n",
    "        print(\"\\nPredicted rate:\")\n",
    "        for label in range(10):\n",
    "            point = np.sum(res[labelmap == label])  # labelの獲得ポイントを集計\n",
    "            count = np.sum(labelmap == label)       # labelの出現頻度をカウント\n",
    "            rate  = point / count                   # 予測レートをを計算\n",
    "            if rate > 0:\n",
    "                print(f\"label = {label}, point = {point}, \"\n",
    "                      f\"count = {count}, rate = {rate:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zmiq1368PJ2W"
   },
   "source": [
    "## SNNネットワーク（SNN Network）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWB1vG8tPIxK"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# SNNネットワーク（BindsNET版）\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SnnNetwork(DiehlAndCook2015):\n",
    "    def __init__(self, Dsat, Hypr):\n",
    "        super().__init__(\n",
    "            n_inpt     = Dsat.inputs,             # 入力層における入力数\n",
    "            n_neurons  = Hypr.neurons,            # ニューロンの数（興奮性、抑制性）\n",
    "            exc        = Hypr.exc,                # シナプスの強度（興奮性層→抑制性層）\n",
    "            inh        = Hypr.inh,                # シナプスの強度（抑制性層→興奮性層）\n",
    "            dt         = Hypr.dt,                 # 時間分解能（ミリ秒）\n",
    "            norm       = Hypr.norm,               # 接続強度の正規化（入力層→興奮性層）（学習に大きく影響する）\n",
    "            inpt_shape = Dsat.shape)              # 入力データの形状\n",
    "        self.shape     = (Hypr.simsteps, 1,) + Dsat.shape\n",
    "        self.classes   = Dsat.classes\n",
    "        self.spikes    = self.__set_monitor(Hypr.simsteps)\n",
    "        self.labelmap  = LabelMap(Hypr.neurons)\n",
    "        self.rates     = torch.zeros((Hypr.neurons, self.classes))\n",
    "        self.Hypr      = Hypr\n",
    "\n",
    "    def __set_monitor(self, simsteps):            # 監視・記録ツールを設定\n",
    "        spikes = {}\n",
    "        for layer in set(self.layers):\n",
    "            spikes[layer] = Monitor(self.layers[layer], state_vars = [\"s\"], time = simsteps)\n",
    "            self.add_monitor(spikes[layer], name=\"%s_spikes\" % layer)\n",
    "        return spikes\n",
    "\n",
    "    def trainmode(self, mode, size):              #\n",
    "        self.train(mode=mode)\n",
    "        self.spike_record = torch.zeros((size, self.Hypr.simsteps, self.Hypr.neurons))\n",
    "\n",
    "    def forward(self, spiket, index):             # 入力層にスパイクトレインを投入する\n",
    "        X = { \"X\": torch.tensor(spiket).view(self.shape) }\n",
    "        self.run(inputs = X, time = self.Hypr.duration)\n",
    "        self.spike_record[index] = self.spikes[\"Ae\"].get(\"s\").squeeze()\n",
    "\n",
    "    def predict(self):                            # 予測\n",
    "        return all_activity(\n",
    "            spikes      = self.spike_record,\n",
    "            assignments = self.labelmap.data,\n",
    "            n_labels    = self.classes)\n",
    "\n",
    "    def mapping(self, labels):                    # ラベルマップ（ニューロンとラベルの対応表）を更新\n",
    "        self.labelmap.data, _, self.rates = assign_labels(\n",
    "            spikes   = self.spike_record,\n",
    "            labels   = torch.tensor(labels),\n",
    "            n_labels = self.classes,\n",
    "            rates    = self.rates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_sIVOX7HbCP"
   },
   "source": [
    "## モデラー（Modeler）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WE9qkY0HblR"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# モデラーの基底クラス\n",
    "#-------------------------------------------------\n",
    "\n",
    "class ModelerBase(metaclass=ABCMeta):\n",
    "    def __init__(self, network, measure, superset): # モデルの構築\n",
    "        self.network  = network\n",
    "        self.measure  = measure\n",
    "        self.superset = superset\n",
    "\n",
    "    @staticmethod\n",
    "    def idx_mini(total_size, batch_size):         # ミニバッチ用のインデックスを作成\n",
    "        whole = np.random.permutation(total_size)\n",
    "        idx_slice = range(0, total_size, batch_size)\n",
    "        return [whole[i:i+batch_size] for i in idx_slice]\n",
    "\n",
    "    @abstractmethod\n",
    "    def training(self, Hypr, verbose=True):       # モデルの訓練\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self):                           # モデルの性能評価\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, processed):                 # 推論（予測）\n",
    "        pass\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ANNモデラー\n",
    "#-------------------------------------------------\n",
    "\n",
    "class AnnModeler(ModelerBase):\n",
    "    def add(self, cols, Fn_init, Fn_actv, Fn_drvt=lambda x:None): # レイヤを追加\n",
    "        funcs = (Fn_init, Fn_actv, Fn_drvt)\n",
    "        self.network.add(cols, funcs)\n",
    "\n",
    "    def training(self, Hypr, verbose=True):       # モデルの訓練\n",
    "        subset = self.superset.train              # トレーニングセットを選択\n",
    "        start = t()\n",
    "        size = len(subset.scaled)\n",
    "        self.measure.init(self, Hypr.epochs, verbose)\n",
    "        self.measure(-1, t()-start)\n",
    "        for epoch in range(Hypr.epochs):\n",
    "            whole = self.idx_mini(size, Hypr.batchsz)\n",
    "            for mini in whole:\n",
    "                self.network.batch(subset, mini)\n",
    "            self.measure(epoch, t()-start)\n",
    "\n",
    "    def evaluate(self):                           # モデルの性能評価\n",
    "        AnnPerform(self)(self.superset.test)      # テストセットを使って性能を評価\n",
    "\n",
    "    def predict(self, scaled):                    # 推論（予測）\n",
    "        return self.network.forward(scaled)\n",
    "\n",
    "#-------------------------------------------------\n",
    "# SNNモデラー\n",
    "#-------------------------------------------------\n",
    "\n",
    "class SnnModeler(ModelerBase):\n",
    "    def training(self, Hypr, verbose=True):       # モデルの訓練\n",
    "        subset = self.superset.train              # トレーニングセットを選択\n",
    "        total = Hypr.extract[0]\n",
    "        start = t()\n",
    "        index = range(total)\n",
    "        self.prov = Provaccu(Hypr.epochs, total)  # 暫定正解率\n",
    "        self.measure.init(self, Hypr.epochs, verbose)\n",
    "        self.measure(-1, t()-start)\n",
    "        for epoch in range(Hypr.epochs):\n",
    "            self.network.trainmode(mode=True, size=Hypr.batchsz)\n",
    "            self.prov.reset(epoch)\n",
    "            labels = []\n",
    "            whole = iter(zip(subset.spiket, subset.label))\n",
    "            for step in range(0, total, Hypr.batchsz):\n",
    "                for i in index[step:step+Hypr.batchsz]:\n",
    "                    (spiket, label) = next(whole)\n",
    "                    self.network.forward(spiket, i % Hypr.batchsz)\n",
    "                    self.network.reset_state_variables()\n",
    "                    labels.append(label)\n",
    "                pred = self.network.predict()[0:len(labels)].numpy()\n",
    "                self.network.mapping(labels)      # Hypr.batchsz回に1回、ラベルマップを更新する\n",
    "                if verbose:\n",
    "                    self.prov.update(pred, np.array(labels))\n",
    "                    self.prov.show_prov_accu(i, t()-start)\n",
    "                labels = []\n",
    "            self.measure(epoch, t()-start)\n",
    "\n",
    "    def __predict(self, spiket):                  # 予測（1件単位）\n",
    "        self.network.forward(spiket, 0)\n",
    "        pred = self.network.predict()[0]\n",
    "        self.network.reset_state_variables()\n",
    "        return pred\n",
    "\n",
    "    def predict(self, spiket):                    # 推論（予測）\n",
    "        self.network.trainmode(mode=False, size=1)\n",
    "        return np.array([self.__predict(st) for st in spiket])\n",
    "\n",
    "    def evaluate(self):                           # モデルの性能評価\n",
    "        SnnPerform(self)(self.superset.test)      # テストセットを使って性能を評価\n",
    "\n",
    "    def reaction(self, spiket):                   # ニューロンの反応を表示\n",
    "        self.network.trainmode(mode=False, size=1)\n",
    "        self.network.forward(spiket, 0)\n",
    "        self.network.reset_state_variables()\n",
    "        resmap = self.network.spike_record.sum(1)[0].numpy()\n",
    "        rect = vector_to_matrix(resmap)                  # 反応マップを矩形に変換\n",
    "        display(pd.DataFrame(rect).replace({None:\"-\"}))  # ニューロンの反応を表示\n",
    "        self.network.labelmap.show_pred_rate(resmap)     # 予測レートを表示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zANQ22grP3tC"
   },
   "source": [
    "# 体験コーナー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RcUAc28hiyh"
   },
   "source": [
    "## ANNの訓練と推論を体験する（digits）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aIZ_zXG6Uvt"
   },
   "outputs": [],
   "source": [
    "class Hypr():\n",
    "    extract  = (300, 50, 50)                      # サブセットの抽出サイズ（train, valid, test）\n",
    "    epochs   = 50                                 # エポック数\n",
    "    eta      = 0.01                               # 学習率\n",
    "    batchsz  = 10                                 # バッチサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNs670Xs6cUA"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "dataset = DatasetDigits()                         # digitsデータセットを選択\n",
    "Dsat = dataset.attr                               # データセットの属性を取得\n",
    "scaler = Scaler().min_max                         # Min-Max正規化を選択\n",
    "superset = AnnSuperset(scaler, Dsat.classes)      # ANNスーパーセットを選択\n",
    "superset.load(dataset, Hypr.extract)              # ANNスーパーセットにデータセットを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DXpnFmX9pLi"
   },
   "outputs": [],
   "source": [
    "sample = SubsetViewer(superset.train, Dsat.shape) # 観察対象にトレーニングセットを指定\n",
    "sample.show_img_data(20, 30)                      # data #20 〜 data #29 のイメージを表示\n",
    "sample.show_raw_data(29, 30)                      # data #29 を表示\n",
    "sample.show_raw_scaled(29, 30)                    # scaled #29 を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzFBHdhK6V4A"
   },
   "outputs": [],
   "source": [
    "network = AnnNetworkMatrix(Dsat.inputs, Hypr.eta) # ANNネットワーク（行列版）を選択\n",
    "measure = AnnMeasure(crossentropy)                # ANNメジャーを選択\n",
    "model = AnnModeler(network, measure, superset)    # ANNモデルを構築\n",
    "model.add(32, xavier_normal, tanh, tanh_der)      # 中間層（for digits）\n",
    "model.add(Dsat.classes, glorot_uniform, softmax)  # 出力層\n",
    "model.training(Hypr)                              # ANNモデルを訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUfYBKrj-Zlj"
   },
   "outputs": [],
   "source": [
    "measure.plot_metrics()                            # 評価指標をプロット\n",
    "measure.show_metrics()                            # 評価指標を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CMS-j4CBRJc"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "model.evaluate()                                  # モデルの性能を評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgTZMeP4jZhm"
   },
   "outputs": [],
   "source": [
    "sys.exit(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BUUnLsDkXlK"
   },
   "source": [
    "## ANNの訓練と推論を体験する（MNIST）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ie16qxO9kdL5"
   },
   "outputs": [],
   "source": [
    "class Hypr():\n",
    "    extract  = (10000, 1000, 100)                 # サブセットの抽出サイズ（train, valid, test）\n",
    "    epochs   = 30                                 # エポック数\n",
    "    eta      = 0.01                               # 学習率\n",
    "    batchsz  = 10                                 # バッチサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZ_KcdEZkgGq"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "dataset = DatasetMNIST()                          # MNISTデータセットを選択\n",
    "Dsat = dataset.attr                               # データセットの属性を取得\n",
    "scaler = Scaler().min_max                         # Min-Max正規化を選択\n",
    "superset = AnnSuperset(scaler, Dsat.classes)      # ANNスーパーセットを選択\n",
    "superset.load(dataset, Hypr.extract)              # ANNスーパーセットにデータセットを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sxfusdAkpce"
   },
   "outputs": [],
   "source": [
    "sample = SubsetViewer(superset.train, Dsat.shape) # 観察対象にトレーニングセットを指定\n",
    "sample.show_img_data(0, 10)                       # data #0 〜 data #9 のイメージを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D39TJTQhkug8"
   },
   "outputs": [],
   "source": [
    "network = AnnNetworkMatrix(Dsat.inputs, Hypr.eta) # ANNネットワーク（行列版）を選択\n",
    "measure = AnnMeasure(crossentropy)                # ANNメジャーを選択\n",
    "model = AnnModeler(network, measure, superset)    # ANNモデルを構築\n",
    "model.add(32, xavier_normal, tanh, tanh_der)      # 中間層（for MNIST）\n",
    "model.add(Dsat.classes, glorot_uniform, softmax)  # 出力層\n",
    "model.training(Hypr)                              # ANNモデルを訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_I2duRWk3x1"
   },
   "outputs": [],
   "source": [
    "measure.plot_metrics()                            # 評価指標をプロット\n",
    "measure.show_metrics()                            # 評価指標を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOpQg5ejk-sV"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "model.evaluate()                                  # モデルの性能を評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6ZgTA42lEfl"
   },
   "outputs": [],
   "source": [
    "sys.exit(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUgXupp8_yld"
   },
   "source": [
    "## ANNの訓練と推論を体験する（Iris）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZesZhQQUAMDh"
   },
   "outputs": [],
   "source": [
    "class Hypr():\n",
    "    extract  = (110, 20, 20)                      # サブセットの抽出サイズ（train, valid, test）\n",
    "    epochs   = 50                                 # エポック数\n",
    "    eta      = 0.01                               # 学習率\n",
    "    batchsz  = 10                                 # バッチサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nmFpCR3AnC5"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "dataset = DatasetIris()                           # Irisデータセットを選択\n",
    "Dsat = dataset.attr                               # データセットの属性を取得\n",
    "scaler = Scaler(axis=0).z_score                   # z-score正規化を選択\n",
    "superset = AnnSuperset(scaler, Dsat.classes)      # ANNスーパーセットを選択\n",
    "superset.load(dataset, Hypr.extract)              # ANNスーパーセットにデータセットを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIFw5YjhZgwz"
   },
   "outputs": [],
   "source": [
    "sample = SubsetViewer(superset.train, Dsat.shape) # 観察対象にトレーニングセットを指定\n",
    "sample.show_raw_data(0, 20)                       # data   #0~#19 を表示\n",
    "sample.show_raw_scaled(0, 20)                     # scaled #0~#19 を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JgU7OCGDpAo"
   },
   "outputs": [],
   "source": [
    "network = AnnNetworkMatrix(Dsat.inputs, Hypr.eta) # ANNネットワーク（行列版）を選択\n",
    "measure = AnnMeasure(crossentropy)                # ANNメジャーを選択\n",
    "model = AnnModeler(network, measure, superset)    # ANNモデルを構築\n",
    "model.add(32, he_normal, relu, relu_der)          # 中間層（for Iris）\n",
    "model.add(Dsat.classes, glorot_uniform, softmax)  # 出力層\n",
    "model.training(Hypr)                              # ANNモデルを訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BW3L8Z1ySFyz"
   },
   "outputs": [],
   "source": [
    "measure.plot_metrics()                            # 評価指標をプロット\n",
    "measure.show_metrics()                            # 評価指標を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FIEQPB2SUr6"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "model.evaluate()                                  # モデルの性能を評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbQuyL2yPkVS"
   },
   "outputs": [],
   "source": [
    "sys.exit(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5iQBMPJX7-j"
   },
   "source": [
    "## SNNの訓練と推論を体験する（digits）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgvdOBBzaP-7"
   },
   "outputs": [],
   "source": [
    "class Hypr():\n",
    "    extract  = (300, 50, 50)                      # サブセットの抽出サイズ（train, valid, test）\n",
    "    epochs   = 10                                 # エポック数\n",
    "    norm     = 6.4                                # 接続強度の正規化（入力層→興奮性層）（学習に大きく影響する）\n",
    "    neurons  = 100                                # ニューロンの数\n",
    "    lamb     = 0.3                                # 基準発火率（λ）\n",
    "    duration = 250                                # シミュレーション期間（ms）\n",
    "    dt       = 1                                  # 時間分解能（ms）\n",
    "    simsteps = int(duration / dt)                 # シミュレーションステップ数\n",
    "    batchsz  = 5                                  # ラベルマップを更新する前のトレース回数\n",
    "    exc      = 22.5                               # シナプスの強度（興奮性層→抑制性層）\n",
    "    inh      = 120                                # シナプスの強度（抑制性層→興奮性層）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xV9TcJ6XaXWF"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "dataset = DatasetDigits()                         # digitsデータセットを選択\n",
    "Dsat = dataset.attr                               # データセットの属性を取得\n",
    "scaler = Scaler().min_max                         # Min-Max正規化を選択\n",
    "encoder = NeuralEncoder(Hypr).poisson_simple      # ポアソンエンコーダ（簡易版）を選択\n",
    "superset = SnnSuperset(scaler, encoder)           # SNNスーパーセットを選択\n",
    "superset.load(dataset, Hypr.extract)              # SNNスーパーセットにデータセットを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zijwjfq-sppV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = SubsetViewer(superset.train, Dsat.shape) # 観察対象にトレーニングセットを指定\n",
    "sample.show_img_data(20, 30)                      # data #20 〜 data #29 のイメージを表示\n",
    "sample.show_raw_data(29, 30)                      # data #29 を表示\n",
    "sample.show_raw_scaled(29, 30)                    # scaled #29 を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAQzvY97aXIf"
   },
   "outputs": [],
   "source": [
    "sample = SpikeViewer(superset.train.spiket[29])\n",
    "sample.show_bit_pattern()                         # ビットパターンを表示\n",
    "sample.show_raster_plot()                         # ラスタープロットを表示\n",
    "sample.show_spike_train(0, 8)                     # スパイクトレインを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVi2rc3UaW2o"
   },
   "outputs": [],
   "source": [
    "network = SnnNetwork(Dsat, Hypr)                  # SNNネットワークを選択\n",
    "measure = SnnMeasure()                            # SNNメジャーを選択\n",
    "model = SnnModeler(network, measure, superset)    # SNNモデルを構築\n",
    "model.training(Hypr)                              # SNNモデルを訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmAJtBes00mE"
   },
   "outputs": [],
   "source": [
    "map = model.network.labelmap                      # ラベルマップを取得\n",
    "map.show_label_map()                              # ラベルマップを表示\n",
    "map.show_label_count()                            # ラベルの出現頻度を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbieEFRYnYH3"
   },
   "outputs": [],
   "source": [
    "measure.plot_metrics()                            # 評価指標をプロット\n",
    "measure.show_metrics()                            # 評価指標を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tX6KRZ1ypeHR"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "model.evaluate()                                  # モデルの性能を評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf_VK_jpG3vn"
   },
   "source": [
    "## ニューロンの反応（digits）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v13kUIM13qtr"
   },
   "outputs": [],
   "source": [
    "model.reaction(superset.test.spiket[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKuqsQd1JKtq"
   },
   "outputs": [],
   "source": [
    "model.reaction(superset.test.spiket[46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zspciUPBR9P7"
   },
   "outputs": [],
   "source": [
    "sys.exit(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5iQBMPJX7-j"
   },
   "source": [
    "## SNNの訓練と推論を体験する（MNIST）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgvdOBBzaP-7"
   },
   "outputs": [],
   "source": [
    "class Hypr():\n",
    "    extract  = (300, 50, 50)                      # サブセットの抽出サイズ（train, valid, test）\n",
    "    epochs   = 10                                 # エポック数\n",
    "    norm     = 78.4                               # 接続強度の正規化（入力層→興奮性層）（学習に大きく影響する）\n",
    "    neurons  = 100                                # ニューロンの数\n",
    "    lamb     = 0.3                                # 基準発火率（λ）\n",
    "    duration = 250                                # シミュレーション期間（ms）\n",
    "    dt       = 1                                  # 時間分解能（ms）\n",
    "    simsteps = int(duration / dt)                 # シミュレーションステップ数\n",
    "    batchsz  = 5                                  # ラベルマップを更新する前のトレース回数\n",
    "    exc      = 22.5                               # シナプスの強度（興奮性層→抑制性層）\n",
    "    inh      = 120                                # シナプスの強度（抑制性層→興奮性層）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xV9TcJ6XaXWF"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "dataset = DatasetMNIST()                          # MNISTデータセットを選択\n",
    "Dsat = dataset.attr                               # データセットの属性を取得\n",
    "scaler = Scaler().min_max                         # Min-Max正規化を選択\n",
    "encoder = NeuralEncoder(Hypr).poisson_simple      # ポアソンエンコーダ（簡易版）を選択\n",
    "superset = SnnSuperset(scaler, encoder)           # SNNスーパーセットを選択\n",
    "superset.load(dataset, Hypr.extract)              # SNNスーパーセットにデータセットを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zijwjfq-sppV"
   },
   "outputs": [],
   "source": [
    "sample = SubsetViewer(superset.train, Dsat.shape) # 観察対象にトレーニングセットを指定\n",
    "sample.show_img_data(0, 10)                       # data #0 〜 data #9 のイメージを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAQzvY97aXIf"
   },
   "outputs": [],
   "source": [
    "sample = SpikeViewer(superset.train.spiket[4])\n",
    "sample.show_bit_pattern()                         # ビットパターンを表示\n",
    "sample.show_raster_plot(fig=(8,6))                # ラスタープロットを表示\n",
    "sample.show_spike_train(116, 126)                 # スパイクトレインを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVi2rc3UaW2o"
   },
   "outputs": [],
   "source": [
    "network = SnnNetwork(Dsat, Hypr)                  # SNNネットワークを選択\n",
    "measure = SnnMeasure()                            # SNNメジャーを選択\n",
    "model = SnnModeler(network, measure, superset)    # SNNモデルを構築\n",
    "model.training(Hypr)                              # SNNモデルを訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmAJtBes00mE"
   },
   "outputs": [],
   "source": [
    "map = model.network.labelmap                      # ラベルマップを取得\n",
    "map.show_label_map()                              # ラベルマップを表示\n",
    "map.show_label_count()                            # ラベルの出現頻度を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbieEFRYnYH3"
   },
   "outputs": [],
   "source": [
    "measure.plot_metrics()                            # 評価指標をプロット\n",
    "measure.show_metrics()                            # 評価指標を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tX6KRZ1ypeHR"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "model.evaluate()                                  # モデルの性能を評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBe-TxS5twUb"
   },
   "outputs": [],
   "source": [
    "sys.exit(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVHLJVQYX_t6"
   },
   "source": [
    "## SNNの訓練と推論を体験する（Iris）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcOGPCADsN8d"
   },
   "outputs": [],
   "source": [
    "class Hypr():\n",
    "    extract  = (110, 20, 20)                      # サブセットの抽出サイズ（train, valid, test）\n",
    "    epochs   = 4                                  # エポック数\n",
    "    norm     = 1.5                                # 接続強度の正規化（入力層→興奮性層）（学習に大きく影響する）\n",
    "    neurons  = 100                                # ニューロンの数\n",
    "    lamb     = 0.3                                # 基準発火率（λ）\n",
    "    duration = 250                                # シミュレーション期間（ms）\n",
    "    dt       = 1                                  # 時間分解能（ms）\n",
    "    simsteps = int(duration / dt)                 # シミュレーションステップ数\n",
    "    batchsz  = 5                                  # ラベルマップを更新する前のトレース回数\n",
    "    exc      = 22.5                               # シナプスの強度（興奮性層→抑制性層）\n",
    "    inh      = 120                                # シナプスの強度（抑制性層→興奮性層）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctDVwMfbseie"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "dataset = DatasetIris()                           # Irisデータセットを選択\n",
    "Dsat = dataset.attr                               # データセットの属性を取得\n",
    "scaler = Scaler(axis=0).min_max                   # Min-Max正規化を選択\n",
    "encoder = NeuralEncoder(Hypr).poisson_simple      # ポアソンエンコーダ（簡易版）を選択\n",
    "superset = SnnSuperset(scaler, encoder)           # SNNスーパーセットを選択\n",
    "superset.load(dataset, Hypr.extract)              # SNNスーパーセットにデータセットを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjnGgC5CsOkF"
   },
   "outputs": [],
   "source": [
    "sample = SubsetViewer(superset.train, Dsat.shape) # 観察対象にトレーニングセットを指定\n",
    "sample.show_raw_data(0, 20)                       # data   #0~#19 を表示\n",
    "sample.show_raw_scaled(0, 20)                     # scaled #0~#19 を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLj8IZJqsOT5"
   },
   "outputs": [],
   "source": [
    "sample = SpikeViewer(superset.train.spiket[0])\n",
    "sample.show_bit_pattern()                         # ビットパターンを表示\n",
    "sample.show_raster_plot(fig=(8,2))                # ラスタープロットを表示\n",
    "sample.show_spike_train(0, 4)                     # スパイクトレインを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2qCNqBbtmAI"
   },
   "outputs": [],
   "source": [
    "network = SnnNetwork(Dsat, Hypr)                  # SNNネットワークを選択\n",
    "measure = SnnMeasure()                            # SNNメジャーを選択\n",
    "model = SnnModeler(network, measure, superset)    # SNNモデルを構築\n",
    "model.training(Hypr)                              # SNNモデルを訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCzLHDh8z3HE"
   },
   "outputs": [],
   "source": [
    "map = model.network.labelmap                      # ラベルマップを取得\n",
    "map.show_label_map()                              # ラベルマップを表示\n",
    "map.show_label_count()                            # ラベルの出現頻度を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4jZbFiptqEb"
   },
   "outputs": [],
   "source": [
    "measure.plot_metrics()                            # 評価指標をプロット\n",
    "measure.show_metrics()                            # 評価指標を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZj_w1SatteP"
   },
   "outputs": [],
   "source": [
    "reset_seed()                                      # 乱数シードをリセット\n",
    "model.evaluate()                                  # モデルの性能を評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBe-TxS5twUb"
   },
   "outputs": [],
   "source": [
    "sys.exit(\"done.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
